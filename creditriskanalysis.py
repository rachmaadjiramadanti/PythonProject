# -*- coding: utf-8 -*-
"""Final Task - PBI.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/17lYD9dGjfSO3CTyFyXsnuMOjJfFnY9lq

# Credit Risk Analysis
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import LabelEncoder
from sklearn.preprocessing import OneHotEncoder
from datetime import datetime
from sklearn.model_selection import StratifiedShuffleSplit
from imblearn.over_sampling import SMOTE
from sklearn.datasets import make_classification
from sklearn.linear_model import LogisticRegression
from xgboost import XGBClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, roc_curve, auc
from sklearn.metrics import confusion_matrix
from sklearn.model_selection import GridSearchCV

"""# IMPORT DATA"""

from google.colab import drive
drive.mount("/content/gdrive")

from google.colab import files
dataset = pd.read_csv('/content/gdrive/MyDrive/Colab Notebooks/loan_data_2007_2014.csv')
dataset = dataset.iloc[:, 1:]
dataset.sample(10)

"""# Data Understanding"""

dataset.info()

dataset.shape

dataset.describe(include=["int64","float64"])

print('Sum Of Funded Amount : ', dataset["funded_amnt"].sum())
print('Sum Of Loan Amount : ', dataset["loan_amnt"].sum())

dataset.describe(include=["object", "bool"])

"""## Duplicate Data Check"""

#Determine if all rows in the dataset are duplicates
if dataset[dataset.duplicated()].empty:
    print("No duplicate rows in DataFrame.")
else:
    print("Duplicate Rows:")
    print(dataset[dataset.duplicated()])

"""There are no duplicate rows in this data. So, each row of this data represents the corresponding borrower ID and member ID for each loan.

## Missing Value Check
"""

def missing_data(data):
    total = data.isnull().sum().sort_values(ascending = False)
    percent = (data.isnull().sum()/data.isnull().count()*100).sort_values(ascending = False)
    return pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])

missing_data(dataset)

"""We can see from this: There is a lot of data missing. So we need to handle these missing values in the next step"""

# Calculate the total missing values per column
total_missing = dataset.isnull().sum()

# Calculate the total missing value and total filled value of the entire dataset
total_missing_all = total_missing.sum()
total_filled_all = dataset.count().sum()

# Data for doughnut diagram
labels = ['Missing', 'Filled']
sizes = [total_missing_all, total_filled_all]
colors = ['#1f77b4', '#ff7f0e']

# Creating a doughnut diagram
fig, ax = plt.subplots()
wedges, texts, autotexts = ax.pie(sizes, labels=labels, autopct='%1.1f%%', startangle=90, colors=colors, wedgeprops=dict(width=0.4))

# Making a circle in the center to create a doughnut diagram
centre_circle = plt.Circle((0, 0), 0.001, fc='white')
fig = plt.gcf()
fig.gca().add_artist(centre_circle)

# Set the proportional aspect so that the doughnut diagram looks like a circle
ax.axis('equal')

# Add text inside the doughnut diagram
for text, autotext in zip(texts, autotexts):
    text.set(size=14, color='black', fontweight='bold', fontfamily='sans-serif')
    autotext.set(size=12, color='black', weight='bold', fontfamily='sans-serif')

plt.title('Percentage of Missing Value and Filled Value', fontdict={'size': 18, 'fontweight': 'bold', 'fontfamily': 'sans-serif'})
plt.show()

"""## View the number of unique values for each column"""

dataset.select_dtypes(include='object').nunique().sort_values(ascending=False)

for col in dataset.select_dtypes(include='object').columns.tolist():
    print(dataset[col].value_counts(normalize=True)*100)
    print('\n')

dataset.select_dtypes(exclude='object').nunique().sort_values(ascending=False)

"""Columns with high cardinality and having only 1 unique value will be discarded in the feature selection steps

# Data Preparation

## Feature Engineering

Categorizing the `addr_state` feature which previously contained the state and then will be categorized or grouped based on the region.
"""

#Categorizing Countries by Region
#Dictionary for regions in the US
us_region_dict = {
    'Northeast': ['CT', 'ME', 'MA', 'NH', 'NJ', 'NY', 'PA', 'RI', 'VT'],
    'Midwest': ['IL', 'IN', 'IA', 'KS', 'MI', 'MN', 'MO', 'NE', 'ND', 'OH', 'SD', 'WI'],
    'South': ['AL', 'AR', 'DE', 'FL', 'GA', 'KY', 'LA', 'MD', 'MS', 'NC', 'OK', 'SC', 'TN', 'TX', 'VA', 'WV'],
    'West': ['AK', 'AZ', 'CA', 'CO', 'HI', 'ID', 'MT', 'NV', 'NM', 'OR', 'UT', 'WA', 'WY']
}
dataset['Region'] = dataset['addr_state'].apply(lambda x: next((region for region, countries in us_region_dict.items() if x in countries), None))
dataset['Region']

"""Classify the interest rate given to customers. Classification is done by dividing into 2 categories, namely low and high interest rates, if the interest rate is less than the median it will be categorized as low and vice versa."""

# Function to classify int rate
def convert_int_rate(int_rate):
    if int_rate < 13.66 :
        return 'Low'
    else:
        return 'High'

dataset['int_rate_int'] = dataset['int_rate'].apply(convert_int_rate)

"""Classify the income of customers/borrowers where this classification is divided into 3 categories. Low income is less than 100,000, medium income is between 100,000 and 200,000, and high income is more than 200,000."""

#Borrower's Income Classification
def classify_income(income):
    if income < 100000:
        return 'Low'
    elif 100000 <= income < 200000:
        return 'Medium'
    else:
        return 'High'

dataset['Klasifikasi_Pendapatan'] = dataset['annual_inc'].apply(classify_income)

"""Convert the employment length feature from a string to an integer data type.

"""

# Function to convert employment_length into integer data type
def convert_employment_length(employment_length):
    if pd.isna(employment_length) or employment_length == 'n/a':
        return None
    elif '< 1 year' in employment_length:
        return 0
    elif '1 year' in employment_length:
        return 1
    elif '10+ years' in employment_length:
        return 10
    else:

# Take a number from a string (example: '2 years' becomes 2)
        return int(''.join(filter(str.isdigit, employment_length)))


dataset['emp_length_int'] = dataset['emp_length'].apply(convert_employment_length)

"""Convert term features that previously had a string data type to integer data"""

#Convert terms from string to numeric data type
dataset['term'] = dataset['term'].map({' 36 months': 36, ' 60 months': 60})
dataset['term'].unique()

"""Classify loan status into 2 categories: Good and Bad.
With rules
Fully Paid, Current, Does not meet the credit policy. Status: Fully Paid is a Good Loan and
Default, Charged Off, Late (31-120 days), In Grace Period, Late (16-30 days), Does not meet the credit policy. Status:Charged Off is Bad Loan
"""

# Conversion of loan status into 2 categories: Good Loan = 0 and Bad Loan = 1
dataset['loan_status'].unique()
def encode_with_custom_labels(column, custom_labels):
    encoded_column = column.map(custom_labels)
    return encoded_column
keterangan = {
    'Fully Paid':0, 'Charged Off':1, 'Current':0, 'Default':1,
       'Late (31-120 days)':1, 'In Grace Period':1, 'Late (16-30 days)':1,
       'Does not meet the credit policy. Status:Fully Paid':0,
       'Does not meet the credit policy. Status:Charged Off':1 }
dataset['loan_condition'] = encode_with_custom_labels(dataset['loan_status'],keterangan)

"""Retrieve the year from all date features in each column"""

# Function to change the date format
def convert_date_format(date_str):
    # Check if the value is a string before parsing
    if isinstance(date_str, str):
        date_object = datetime.strptime(date_str, "%b-%y")
        return date_object.strftime("%Y")
    else:
        return None
dataset['issue_d'] = dataset['issue_d'].apply(convert_date_format)
dataset['last_pymnt_d'] = dataset['last_pymnt_d'].apply(convert_date_format)
dataset['next_pymnt_d'] = dataset['next_pymnt_d'].apply(convert_date_format)
dataset['last_credit_pull_d'] = dataset['last_credit_pull_d'].apply(convert_date_format)
dataset['earliest_cr_line'] = dataset['earliest_cr_line'].apply(convert_date_format)

"""## Data Visualization"""

# Grouping by year and calculating the average loan amount
average_loan_amount_by_year = dataset.groupby('issue_d')['loan_amnt'].mean().reset_index()

# Finding the year with the highest average loan amount
max_loan_year = average_loan_amount_by_year.loc[average_loan_amount_by_year['loan_amnt'].idxmax(), 'issue_d']

# Plotting the bar chart with a different color for the year with the highest average loan amount
plt.figure(figsize=(12, 8))
ax = sns.barplot(x='issue_d', y='loan_amnt', data=average_loan_amount_by_year, palette=['tab:blue' if year != max_loan_year else 'tab:red' for year in average_loan_amount_by_year['issue_d']])
plt.title('Issuance of Loans Over the Years', fontsize=16)
plt.xlabel('Year', fontsize=14)
plt.ylabel('Average Loan Amount Issued', fontsize=14)

# Adding a legend
legend_labels = ['Highest Average Loan Amount' if year == max_loan_year else 'Other Years' for year in average_loan_amount_by_year['issue_d']]
ax.legend(handles=[plt.Line2D([0], [0], color='tab:red', lw=4)], labels=legend_labels, loc='upper left')

plt.show()

# Calculate loan amount based on status
loan_status_counts = dataset['loan_status'].value_counts()

# Plot bar chart
plt.figure(figsize=(10, 6))
sns.barplot(y=loan_status_counts.index, x=loan_status_counts, palette='viridis')
plt.title('Distribution of Loan Status')
plt.ylabel('Loan Status')
plt.xlabel('Number of Loans')
plt.xticks(rotation=45, ha='right')  # Rotate x-axis labels for better readability
plt.show()

# Filter the dataset for specific loan statuses
selected_loan_statuses = ['Charged Off', 'Default', 'Late (31-120 days)', 'In Grace Period', 'Late (16-30 days)', 'Does not meet the credit policy. Status:Charged Off']
filtered_data = dataset[dataset['loan_status'].isin(selected_loan_statuses)]

# Group by 'Region' and 'loan_status', and count the occurrences
loan_counts = filtered_data.groupby(['Region', 'loan_status']).size().unstack()

# Plot the stacked bar chart
fig, ax = plt.subplots(figsize=(10, 6))
loan_counts.plot(kind='bar', stacked=True, ax=ax, )

# Add labels and title
plt.xlabel('Region')
plt.ylabel('Number of Loans')
plt.title('Bad Loan Status Composition by Region')

# Add legend outside the plot for better visibility
plt.legend(title='Loan Status', bbox_to_anchor=(1.05, 1), loc='upper left')

# Display the plot
plt.show()

import plotly.graph_objs as go
from plotly.subplots import make_subplots
# Assume 'loan_status' is the column name for the loan status in your DataFrame
fatal_loan_statuses = ['Charged Off', 'Default', 'Late (31-120 days)', 'In Grace Period', 'Late (16-30 days)', 'Does not meet the credit policy. Status:Charged Off']

# Filter the DataFrame based on the fatal loan statuses
fatal_loans_df = dataset[dataset['loan_status'].isin(fatal_loan_statuses)]

# Calculate metrics based on loan status for each state
metrics_df = fatal_loans_df.groupby(['addr_state', 'Region']).agg({'issue_d': 'count',
                                                                  'int_rate': 'mean',
                                                                  'annual_inc': 'mean'}).reset_index()

# Convert metrics to string for display in the hover text
for col in metrics_df.columns:
    metrics_df[col] = metrics_df[col].astype(str)

# Define color scale with shades of red
scl = [[0.0, 'rgb(255, 230, 230)'], [0.2, 'rgb(255, 179, 179)'], [0.4, 'rgb(255, 128, 128)'],
       [0.6, 'rgb(255, 77, 77)'], [0.8, 'rgb(255, 26, 26)'], [1.0, 'rgb(204, 0, 0)']]

# Create text for hover display
metrics_df['text'] = metrics_df['addr_state'] + '<br>' + \
                     'Region: ' + metrics_df['Region'] + '<br>' + \
                     'Average loan interest rate: ' + metrics_df['int_rate'] + '<br>' + \
                     'Average annual income: ' + metrics_df['annual_inc']

# Plotly choropleth map
fig = make_subplots(rows=1, cols=1)

fig.add_trace(go.Choropleth(
    colorscale=scl,
    autocolorscale=False,
    locations=metrics_df['addr_state'],
    z=metrics_df['issue_d'],
    locationmode='USA-states',
    text=metrics_df['text'],
    marker=dict(
        line=dict(
            color='rgb(255,255,255)',
            width=2
        )),
    colorbar=dict(
        title="Number of Fatal Loans")
))

fig.update_layout(
    title_text='Distribution of Bad Credit Conditions in Each State',
    geo=dict(
        scope='usa',
        projection=dict(type='albers usa'),
        showlakes=True,
        lakecolor='rgb(255, 255, 255)')
)

fig.show()

# Create a boxplot for 'home_ownership' and 'loan_amnt'
plt.figure(figsize=(12, 8))
sns.barplot(x='home_ownership', y='loan_amnt', data=dataset, palette='viridis')
plt.title('Loan Amount Distribution by Home Ownership', fontsize=16)
plt.xlabel('Home Ownership', fontsize=14)
plt.ylabel('Loan Amount', fontsize=14)
plt.show()

# Membuat boxplot menggunakan Seaborn
plt.figure(figsize=(10, 6))
sns.boxplot(x='Klasifikasi_Pendapatan', y='loan_amnt', data=dataset)
plt.title('Distribution of loan amount based on income')
plt.xlabel('Income Classification')
plt.ylabel('Loan Amount')

# Menampilkan plot
plt.show()

top5_purposes = dataset['purpose'].value_counts().nlargest(5).sort_values(ascending = False)

# Buat grafik batang horizontal
plt.figure(figsize=(12, 8))
sns.barplot(x=top5_purposes, y=top5_purposes.index, palette='viridis')
plt.title('Top 5 Loan Purposes', fontsize=16)
plt.xlabel('Count', fontsize=14)
plt.ylabel('Loan Purpose', fontsize=14)
plt.show()

# Membuat DataFrame baru dengan kolom 'issue_d' sebagai tahun dan 'loan_condition'
loan_condition_by_year = dataset.groupby(['issue_d', 'loan_condition']).size().unstack()

# Menghitung persentase
loan_condition_by_year_percentage = loan_condition_by_year.div(loan_condition_by_year.sum(axis=1), axis=0) * 100

# Membuat grafik stacked bar column
plt.figure(figsize=(12, 8))
sns.barplot(x=loan_condition_by_year_percentage.index, y=loan_condition_by_year_percentage[0], label='Good', color='green')
sns.barplot(x=loan_condition_by_year_percentage.index, y=loan_condition_by_year_percentage[1], label='Bad', color='red', bottom=loan_condition_by_year_percentage[0])
plt.title('Loan Condition Proportion by Year', fontsize=16)
plt.xlabel('Year', fontsize=14)
plt.ylabel('Percentage', fontsize=14)
plt.legend(title='Loan Condition', title_fontsize='12')
plt.show()

# Membuat boxplot menggunakan Seaborn
plt.figure(figsize=(10, 6))
sns.boxplot(x='Klasifikasi_Pendapatan', y='int_rate', data=dataset)
plt.title('Interest rate distribution by income')
plt.xlabel('Income Clasification')
plt.ylabel('Interest Rate')

# Menampilkan plot
plt.show()

# Urutkan DataFrame berdasarkan klasifikasi pendapatan
sorted_dataset = dataset.sort_values(by='Klasifikasi_Pendapatan', key=lambda x: x.map({'Low': 0, 'Medium': 1, 'High': 2}))

# Hitung jumlah pinjaman berdasarkan klasifikasi pendapatan dan durasi pekerjaan
loan_counts = sorted_dataset.groupby(['Klasifikasi_Pendapatan', 'emp_length_int']).size().unstack()

# Plot bar chart
fig, ax = plt.subplots(figsize=(10, 6))
loan_counts.plot(kind='bar', stacked=True, ax=ax, cmap='viridis')

# Menambahkan label dan judul
plt.xlabel('Income Classification')
plt.ylabel('Count')
plt.title('Composition of length of service by income')

# Menambahkan legenda dengan penyesuaian
plt.legend(title='Employment Length', bbox_to_anchor=(1.05, 1), loc='upper left')

# Menampilkan plot
plt.show()

# Hitung jumlah pinjaman berdasarkan klasifikasi pendapatan dan kondisi pinjaman
loan_condition_by_income = dataset.groupby(['Klasifikasi_Pendapatan', 'loan_condition']).size().unstack()

# Hitung proporsi
loan_condition_by_income_percentage = loan_condition_by_income.div(loan_condition_by_income.sum(axis=1), axis=0) * 100

# Plot stacked bar chart
plt.figure(figsize=(12, 8))
sns.barplot(data=loan_condition_by_income_percentage.reset_index(), x='Klasifikasi_Pendapatan', y=0, color='green', label='Good', )
sns.barplot(data=loan_condition_by_income_percentage.reset_index(), x='Klasifikasi_Pendapatan', y=1, color='red', label='Bad',bottom=loan_condition_by_income_percentage[0])

# Menambahkan label dan judul
plt.xlabel('Income Clasification')
plt.ylabel('Percentage')
plt.title('Percentage Proportion of Loan Conditions at Each Income Level')
plt.legend(title='Loan Condition', bbox_to_anchor=(1.05, 1), loc='upper left')

# Menampilkan plot
plt.show()

# Misalkan 'loan_status' adalah nama kolom kondisi pinjaman dalam DataFrame Anda
loan_status_counts = dataset['loan_condition'].value_counts()

# Membuat pie chart
plt.figure(figsize=(8, 8))
plt.pie(loan_status_counts, labels=['Good Loan', 'Bad Loan'], autopct='%1.1f%%', startangle=140, colors=['green','red'])
plt.title('Distribution of Loan Condition')
plt.show()

"""Dari gambar diatas terlihat bahwa status pinjaman lebih condong ke satu kondisi yaitu kondisi good loan dimana status pinjaman good loan memiliki presentase sebesar 88.1% sedangkan status pinjaman bad loan memiliki presentase 11.9%.

# Feature Selection
"""

#Column to drop
columndrop = [
    #unique id
    'id', 'member_id',
    #free text
    'url','desc','title',
    #fitur yang sudah diklasifikasi sebelumnya
    'loan_status','emp_length','annual_inc','addr_state',
    #fitur yang tidak diperlukan
    'zip_code',
    #fitur dengan high cardinality
    'purpose','title','emp_title',
    #fitur dengan satu nilai unik
    'application_type','policy_code'
]

dataset = dataset.drop(columns = columndrop)
dataset.info()

### Hapus Kolom yang memiliki banyak missing value
def remove_columns_with_high_missing_values(df, threshold):
    # Hitung total nilai yang hilang untuk setiap kolom
    missing_values = df.isnull().sum()

    # Urutkan kolom berdasarkan total nilai yang hilang secara menurun
    sorted_columns = missing_values.sort_values(ascending=False)

    # Pilih kolom-kolom dengan total nilai yang hilang tinggi
    high_missing_columns = sorted_columns[sorted_columns > threshold].index

    # Hapus kolom-kolom tersebut dari DataFrame
    df_filtered = df.drop(columns=high_missing_columns)

    return df_filtered

dataset = remove_columns_with_high_missing_values(dataset,0.4*len(dataset))
dataset.info()

missing_data(dataset)

#Handling Missing Value
dataset = dataset.apply(lambda x: x.fillna(x.mode()[0]))

dataset = dataset.astype({'issue_d': 'int', 'last_pymnt_d': 'int', 'last_credit_pull_d': 'int', 'earliest_cr_line': 'int'})
dataset.info()

# Menghitung total missing value per kolom
total_missing = dataset.isnull().sum()

# Menghitung total missing value dan total filled value seluruh dataset
total_missing_all = total_missing.sum()
total_filled_all = dataset.count().sum()

# Data untuk diagram donat
labels = ['Missing', 'Filled']
sizes = [total_missing_all, total_filled_all]
colors = ['#1f77b4', '#ff7f0e']  # Warna biru dongker dan oranye

# Membuat diagram donat
fig, ax = plt.subplots()
wedges, texts, autotexts = ax.pie(sizes, labels=labels, autopct='%1.1f%%', startangle=120, colors=colors, wedgeprops=dict(width=0.4))

# Membuat lingkaran di tengah untuk membuat diagram donat
centre_circle = plt.Circle((0, 0), 0.001, fc='white')
fig = plt.gcf()
fig.gca().add_artist(centre_circle)

# Atur aspek proporsional agar diagram donat terlihat seperti lingkaran
ax.axis('equal')

# Menambahkan tulisan di dalam diagram donat
for text, autotext in zip(texts, autotexts):
    text.set(size=14, color='black', fontweight='bold', fontfamily='sans-serif')
    autotext.set(size=12, color='black', weight='bold', fontfamily='sans-serif')

# Menampilkan diagram donat
plt.title('Persentase Missing Value dan Filled Value', fontdict={'size': 18, 'fontweight': 'bold', 'fontfamily': 'sans-serif'})
plt.show()

"""# Procesing Data"""

#Mempersiapkan data untuk modeling menggunakan Machine Learning
#Labeling fitur dengan tipe data string
label_encoder = LabelEncoder()
dataset['grade'] = label_encoder.fit_transform(dataset['grade'])
dataset['sub_grade'] = label_encoder.fit_transform(dataset['sub_grade'])
dataset['verification_status'] = label_encoder.fit_transform(dataset['verification_status'])
dataset['initial_list_status'] = label_encoder.fit_transform(dataset['initial_list_status'])
dataset['int_rate_int'] = label_encoder.fit_transform(dataset['int_rate_int'])
dataset.info()

nominal_cols = ['home_ownership','pymnt_plan', 'Region','Klasifikasi_Pendapatan']
# Membuat objek OneHotEncoder
encoder = OneHotEncoder()

# Melakukan fit-transform pada kolom kategori
one_hot_encoded = pd.DataFrame(encoder.fit_transform(dataset[nominal_cols]).toarray(), columns=encoder.get_feature_names_out(nominal_cols))

# Output DataFrame setelah One-Hot Encoding
print(one_hot_encoded)

df = dataset.drop(nominal_cols, axis=1)
df_clear = pd.concat([df,one_hot_encoded],axis=1)

df_clear.info()

"""## Splitting Data

Memisahkan antara input dan target kolom (loan_condition)
"""

X = df_clear.drop(['loan_condition'], axis=1)
y = df_clear['loan_condition']

"""Melakukan splitting data menggunakan stratified shuffle split. Metode Stratified Random Split dapat mengurangi bias
pada data. Metode ini dapat mengurangi peluang data random yang dipilih hanya memiliki satu kondisi peminjaman saja yaitu data pinjaman good loan.
Pembagian data ini dilakukan dengan membagi 80% data sebagai training dan 20% sebagai testing
"""

stratified_splitter = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)

# Melakukan split dan mendapatkan indeks untuk training set dan testing set
for train_index, test_index in stratified_splitter.split(X, y):
    X_train, X_test = X.iloc[train_index], X.iloc[test_index]
    y_train, y_test = y.iloc[train_index], y.iloc[test_index]

"""## Handling Imbalance - SMOTE

Karena komposisi data tidak seimbang yaitu data lebih banyak atau lebih condong ke satu kondisi yaitu good loan. Sehingga perlu dilakukan resampling pada data. Metode yang dipilih untuk melakukan resampling adalah metode SMOTE dimana metode ini termasuk kedalam Teknik Over Sampling. SMOTE bekerja dengan membuat sampel sintetis baru untuk kelas minoritas, sehingga menyeimbangkan distribusi kelas dalam dataset. Proses SMOTE melibatkan pembentukan sampel baru yang berada di antara sampel-sampel minoritas yang sudah ada.
"""

smote = SMOTE(sampling_strategy='auto', random_state=42)  # 'auto' menyesuaikan jumlah sampel dengan kelas minoritas
X_resampled, y_resampled = smote.fit_resample(X_train, y_train)
print(pd.Series(y_resampled).value_counts())

# Misalkan 'loan_status' adalah nama kolom kondisi pinjaman dalam DataFrame Anda
loan_status_counts = y_resampled.value_counts()

# Membuat pie chart
plt.figure(figsize=(8, 8))
plt.pie(loan_status_counts, labels=['Good Loan', 'Bad Loan'], autopct='%1.1f%%', startangle=180, colors=['green','red'])
plt.title('Distribution of Loan Condition')
plt.show()

"""# Data Modeling

## Building Model Training

Model yang saya gunakan dalam project Klasifikasi ini adalah Logistic Regression dan XGBoost, Dimana akan dilihat model mana yang performancenya lebih bagus dengan parameter yang ditentukan.
"""

# Fungsi untuk melatih dan mengevaluasi model Logistic Regression
def train_and_evaluate_logistic_regression(X_train, y_train, X_test, y_test, max_iter):
    model = LogisticRegression(max_iter=max_iter)
    model.fit(X_train, y_train)

    y_pred = model.predict(X_test)
    accuracy = accuracy_score(y_test, y_pred)
    precision = precision_score(y_test, y_pred)
    recall = recall_score(y_test, y_pred)
    f1 = f1_score(y_test, y_pred)
    roc_auc = roc_auc_score(y_test, model.predict_proba(X_test)[:, 1])

    # Plot ROC curve
    fpr, tpr, _ = roc_curve(y_test, model.predict_proba(X_test)[:, 1])
    roc_auc = auc(fpr, tpr)
    plt.figure()
    plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = {:.2f})'.format(roc_auc))
    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
    plt.xlabel('False Positive Rate')
    plt.ylabel('True Positive Rate')
    plt.title('ROC Curve Logistic Regression')
    plt.legend(loc='lower right')
    plt.show()
    hasil = {
        'Model' : 'Logistic Regression' ,
        'y_pred': y_pred,
        'Accuracy': accuracy,
        'Precision': precision,
        'Recall': recall,
        'F1-score': f1,
        'AUC ROC': roc_auc
    }

    return hasil
# Fungsi untuk melatih dan mengevaluasi model XGBoost
def train_and_evaluate_xgboost(X_train, y_train, X_test, y_test, n_estimators):
    model = XGBClassifier(n_estimators=n_estimators)
    model.fit(X_train, y_train)

    y_pred = model.predict(X_test)
    accuracy = accuracy_score(y_test, y_pred)
    precision = precision_score(y_test, y_pred)
    recall = recall_score(y_test, y_pred)
    f1 = f1_score(y_test, y_pred)
    roc_auc = roc_auc_score(y_test, model.predict_proba(X_test)[:, 1])

    # Plot ROC curve
    fpr, tpr, _ = roc_curve(y_test, model.predict_proba(X_test)[:, 1])
    roc_auc = auc(fpr, tpr)
    plt.figure()
    plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = {:.2f})'.format(roc_auc))
    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
    plt.xlabel('False Positive Rate')
    plt.ylabel('True Positive Rate')
    plt.title('ROC Curve XGBoost')
    plt.legend(loc='lower right')
    plt.show()
    hasil = {
        'Model' : 'XGBoost',
        'y_pred': y_pred,
        'Accuracy': accuracy,
        'Precision': precision,
        'Recall': recall,
        'F1-score': f1,
        'AUC ROC': roc_auc
    }

    return hasil

"""## Evaluation Model"""

#Evaluasi pada data training
# Inisialisasi nilai yang akan diuji
max_iter_values = [50, 100, 200]
n_estimators_values_xgb = [50, 100, 200]

# List untuk menyimpan hasil eksperimen
results = []

# Lakukan eksperimen untuk Logistic Regression
for max_iter in max_iter_values:
    results.append(train_and_evaluate_logistic_regression(X_resampled, y_resampled, X_resampled, y_resampled, max_iter))

# Lakukan eksperimen untuk XGBoost
for n_estimators in n_estimators_values_xgb:
    results.append(train_and_evaluate_xgboost(X_resampled, y_resampled, X_resampled, y_resampled, n_estimators))

# Buat DataFrame dari hasil eksperimen
results_df = pd.DataFrame(results)

# Tampilkan DataFrame
print(results_df)

#Evaluasi pada data testing
# Inisialisasi nilai yang akan diuji
max_iter_values = [50, 100, 200]
n_estimators_values_xgb = [50, 100, 200]

# List untuk menyimpan hasil eksperimen
results = []

# Lakukan eksperimen untuk Logistic Regression
for max_iter in max_iter_values:
    results.append(train_and_evaluate_logistic_regression(X_resampled, y_resampled, X_test, y_test, max_iter))

# Lakukan eksperimen untuk XGBoost
for n_estimators in n_estimators_values_xgb:
    results.append(train_and_evaluate_xgboost(X_resampled, y_resampled, X_test, y_test, n_estimators))

# Buat DataFrame dari hasil eksperimen
results_df = pd.DataFrame(results)

# Tampilkan DataFrame
print(results_df)

"""Dari hasil di atas terlihat bahwa evaluasi pada data training dan data testing menunjukan bahwa model bagus dan tidak overfitting atau under fitting di lihat dari nilai akurasi yang dihasilkan pada data training meningkat seiring bertambahnya parameter (max_iter dan n_estimator) dan pada data testing pun juga sama.
Model Logistik Regression pada iterasi tersebut belum mencapai titik konvergen, jika dibandingkan XGBoost. XGBoost lebih baik dibandingkan dengan Logistic Regression.

Jika dilihat dari nilai f1 score, terlihat bahwa model XGBoost dengan n_estimator = 200 memiliki nilai f1 score yang paling tinggi diantara model yang lain. Sehingga model yang paling baik adalah model XGBoost dengan n_estimator = 200. Sehingga model yang akan dipilih dan digunakan  dalam proses prediksi selanjutnya

## Prediksi
"""

result = train_and_evaluate_xgboost(X_resampled, y_resampled, X_test, y_test, n_estimators = 200)
# Mengakses y_pred, accuracy, dan f1_score
predictions = result['y_pred']
accuracy = result['Accuracy']
f1 = result['F1-score']
cm = confusion_matrix(y_test, predictions)
print("Confusion Matrix:")
print(cm)

# Plot confusion matrix dengan seaborn
plt.figure(figsize=(6, 6))
sns.heatmap(cm, annot=True, fmt=".3f", linewidths=.5, square=True, cmap="Blues")
plt.ylabel('Actual label')
plt.xlabel('Predicted label')
plt.title('Confusion Matrix')
plt.show()

"""Dari confusion matrix disamping terlihat bahwa dari 78249 nasabah dengan status kredit baik, model memprediksi ada 120 nasabah yang diprediksi memiliki status kredit buruk. Kemudian dari 10439 nasabah dengan status kredit buruk, model memprediksi ada 1951 nasabah yang diprediksi memiliki status kredit baik. Dengan akurasi yang diperoleh yaitu 97,66% dan f1 score 89,12%

"""

print("Hasil Prediksi:")
print(list(predictions))

print("Akurasi:", accuracy)
print("F1-score:", f1)

# Menghitung jumlah setiap kategori
counts = {status: list(predictions).count(status) for status in set(list(predictions))}
print(counts)

# Membuat plot
plt.figure(figsize=(8, 6))
plt.bar(['Good','Bad'], counts.values(), color=['green', 'red'])  # Warna hijau untuk 'Approved' dan merah untuk 'Rejected'
plt.title('Count Plot for Loan Status')
plt.xlabel('Loan Status')
plt.ylabel('Count')
plt.show()

"""Dapat dilihat setelah dilakukan prediksi menggunakan XGBoost pada data test, diperoleh perbandingan debitur yang tergolong good dan bad, Dimana terdapat 80080 nasabah yang termasuk kedalam kategori good atau status kredit baik, dan ada 8608 nasabah yang termasuk kedalam kategori bad atau status kredit buruk.

Dengan demikian pada kasus ini nasabah dengan status kredit baik lebih mendominasi dibandingkan nasabah dengan status kredit buruk

"""

